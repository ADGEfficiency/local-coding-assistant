{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e22061",
   "metadata": {},
   "source": [
    "Original notebook - https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing\n",
    "Installs Unsloth, Xformers (Flash Attention) and all other packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b18e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "!pip install wandb -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdab8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import datasets\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from google.colab import drive\n",
    "import wandb\n",
    "import shutil\n",
    "\n",
    "import pathlib\n",
    "\n",
    "output_dir = pathlib.Path(\"./drive/MyDrive/fine-tune-codellama/\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"Fine tuning Codellama\", job_type=\"training\", anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a2ffd",
   "metadata": {},
   "source": [
    "# Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682f602",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model = \"unsloth/codellama-7b-bnb-4bit\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0ec81",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72debed6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples: dict):\n",
    "    prompt_responses = []\n",
    "    for example in examples[\"prompt-response\"]:\n",
    "        prompt_responses.append(\n",
    "            example + tokenizer.eos_token,\n",
    "        )\n",
    "    return {\"prompt-responses\": prompt_responses}\n",
    "\n",
    "\n",
    "dataset = datasets.load_dataset(\"adgefficiency/energy-py-linear\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "dataset_te = datasets.load_dataset(\"adgefficiency/energy-py-linear\", split=\"test\")\n",
    "dataset_te = dataset_te.map(formatting_prompts_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c7ad9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ac3b8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset_te,\n",
    "    dataset_text_field=\"prompt-response\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        # max_steps=2,  # Set num_train_epochs = 1 for full training runs\n",
    "        num_train_epochs=2,\n",
    "        save_steps=100,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=output_dir,\n",
    "        report_to=\"wandb\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd7ed7",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde39c53",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "    [dataset[\"prompt\"][0]],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "for response in decoded_outputs:\n",
    "    print(f\"{response=}\")\n",
    "    print(f\"{dataset['output'][0]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a15644",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b4943",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5beb3",
   "metadata": {},
   "source": [
    "# GGUF / llama.cpp Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d894a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q5_k_m\")\n",
    "    source_path = \"model/unsloth.Q5_K_M.gguf\"\n",
    "    destination_path = output_dir / source_path\n",
    "    shutil.copy(source_path, destination_path)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
